config: TBD  # config file name, will be overwritten by sacred

special:
  debug: null  # 'one_iter', 'one_epoch', null for no debug
  save_current_project: False  # maybe always False in inference
  print_save_config_ignore: [sweep]
  logger_config_ignore: [env, info, sweep, special]
  extra_name: null  # null for no extra name

data:
  batch_size_per_rank: 1

tester:
  breath_time: 0.0
  tester_choice: default
  use_best: True
  save_images: True
  checkpoint_path: TBD
  train_cfg_path: ./outputs/2023-11-01-13-19-19_template_simple_default_mse_32=32_1_1/cfg.yaml  # XXX: required

info:
  task_type: Infer  # Test, Infer, ...
  wandb:
    wandb_enabled: True
    wandb_resume_enabled: False  # if True, wandb run will have the same ID when resuming an existing training work
    wandb_watch_model: False  # do not use
    wandb_watch_freq: 100
    wandb_buffer_time: 300  # seconds
  tensorboard:
    tensorboard_enabled: True
    # tensorboard_graph: True  # NO NEED
  # iter_log_freq: 10  # NO NEED. only one epoch when inference
  cli_log_freq: 1
  # global_tqdm: True  # NO NEED. only one epoch
  # torchinfo: True  # NO NEED. print model info to log.txt
  # print_param_names: True  # NO NEED.

env:
  device: cuda
  gpu: TBD
  distributed: TBD
  dist_url: env://
  dist_backend: TBD
  world_size: TBD
  rank: TBD
  num_workers: 4
  infinite_dataloader: False

sweep:
  sweep_enabled: False
  sweep_params:  # use '-' as connect symbols for sub-params 
    data-batch_size_per_rank: [1, 2]
