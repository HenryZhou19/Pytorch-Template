config:  # main config (in this file) overrides additional configs
  additional:  # add additional config file paths --- the order matters (later ones overwrite previous ones, then the main config (this file) overwrites them, and finally the command line arguments overwrite all)
  - configs/defaults/shared.yaml
  - configs/defaults/train.yaml

special:
  extra_name: template_multi_cycle  # null for no extra name

info:
  output_dir: ./outputs/template/
  project_name: template

data:
  dataset: simple
  split_rate: 0.8

model:
  model_choice: simple
  backbone: default
  ema:
    ema_enabled: True

criterion:
  criterion_choice: default  # default as 'model.model_choice'
  loss: mse
  primary_criterion: null  # null (None) to use loss as primary_criterion
  primary_criterion_higher_better: False  # XXX: important for choosing best model

trainer:
  ## batch_control
  trainer_batch_size_per_rank: 32
  sync_lr_with_batch_size: 0  # XXX: if > 0, sync lr with batchsize (lr_real = lr_config * batch_size_total[all_ranks, grad_accumulation] / sync_lr_with_batch_size)
  ## batch_control ends
  trainer_choice: multi_cycle
  pretrained_models: null  # None or a dict of pretrained models {name1: path1, name2: path2, ...}
  load_from_ema: True
  epochs: 30
  optimizer:
    optimizer_choice: adamw  # adamw, sgd
    sgd_momentum: 0.9
    lr_default: 4.0e-4
    wd_default: 1.0e-2
    # param_groups:  # xx_name means if string 'name' in named_parameters, use these xx settings
    #   lr_backbone: 4.0e-4
    #   wd_backbone: 1.0e-2
    #   lr_head: 1.0e-4
    #   wd_head: 1.0e-4
    max_grad_norm: 1.0  # <= 0 means no gradient clipping
    freeze_modules: []  # [submodule_name1, submodule_name2, ...], '.' can be used to point to a subsubmodule
    scheduler:
      scheduler_choice: cosine_multi_cycle  # multistep, cosine, linear, cosine_restart, cosine_multi_cycle
      # lr_milestones_epochs: null  # only for scheduler_choice == 'multistep' [50, 100]
      # lr_milestones_steps: null # only for scheduler_choice == 'multistep', if lr_milestones_steps is not None, override lr_milestones_epochs
      # lr_decay_gamma: 0.1  # only for scheduler_choice == 'multistep'
      # lr_first_cycle_epochs: null  # only for scheduler_choice == 'cosine_restart'
      # lr_first_cycle_steps: null  # only for scheduler_choice == 'cosine_restart'
      # lr_cycle_mult: 1.0  # only for scheduler_choice == 'cosine_restart'
      lr_cycle_gamma: 0.5  # only for scheduler_choice == 'cosine_restart or cosine_multi_cycle'
      lr_cycle_epochs_list: [2, 5]  # only for scheduler_choice == 'cosine_multi_cycle'
      warmup_epochs: 10
      warmup_steps: 50  # if warmup_steps >= 0, override warmup_epochs
      lr_min_factor: 0.1  # default
      # warmup_type: linear  # default
  cycle_modules_list: [[backbone], [head]]  # only for trainer_choice == 'multi_cycle'
  copy_ema_after_each_cycle: True  # only for trainer_choice == 'multi_cycle'
  min_hold_memory_mb: 0

sweep:
  sweep_enabled: False
  sweep_params:  # use '//' as the connector for sub-params
    trainer//optimizer//lr_default: [2.0e-4, 4.0e-4]
    trainer//optimizer//scheduler//scheduler_choice: [linear, cosine]
