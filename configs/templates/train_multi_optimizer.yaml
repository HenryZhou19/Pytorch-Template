config:  # main config (in this file) overrides additional configs
  additional:  # add additional config file paths --- the order matters (later ones overwrite previous ones, then the main config (this file) overwrites them, and finally the command line arguments overwrite all)
  - configs/defaults/shared.yaml
  - configs/defaults/train_multi_optimizer.yaml

special:
  extra_name: template_multi_optimizer  # null for no extra name

info:
  output_dir: ./outputs/template/
  project_name: template

data:
  dataset: mnist
  split_rate: 0.8

model:
  model_choice: lenet_multi_optimizer_v2
  backbone: default

criterion:
  criterion_choice: lenet_multi_optimizer  # default as 'model.model_choice'
  loss: ce
  primary_criterion: loss_fcs  # null (None) to use loss as primary_criterion
  primary_criterion_higher_better: False  # XXX: important for choosing best model

trainer:
  ## batch_control
  trainer_batch_size_per_rank: 64
  sync_lr_with_batch_size: 64  # XXX: if > 0, sync lr with batchsize (lr_real = lr_config * batch_size_total[all_ranks, grad_accumulation] / sync_lr_with_batch_size)
  ## batch_control ends
  trainer_choice: default
  epochs: 20
  real_epochs: null  # if not None, do early stopping based on real_epochs (scheduler is still based on epochs)
  optimizer_1:
    identifier: convs  # just the name for the module in the root_module
    optimizer_choice: adamw
    lr_default: 4.0e-4
    wd_default: 1.0e-2
    max_grad_norm_name: [all]  # names for grad norm groups in loggers
    max_grad_norm_value: [1.0]  # .inf means no gradient clipping (setting to 0.0 is the same, but grad_norm will not be logged)
    max_grad_norm_modules: [['']]  # nested_list: exact module name for this optimizer's root module, '' means the whole model, or a list of submodule names [submodule_name1, submodule_name2, ...]
    lr_scheduler:
      ### len(phase) == N, e.g. N=2: warmup, keep
      lr_phase_epochs: [1]  # (N-1) the last phase goes to the end
      lr_phase_steps: null  # (N-1) the last phase goes to the end; if phase_steps is not null, override phase_epochs
      lr_phase_scales: [0.0, 1.0, 1.0]  # (N+1) start scale of each phase and the final scale of the end
      lr_phase_types: [linear, keep_start]  # (N)
    wd_scheduler:
      ### len(phase) == N, e.g. N=2: warmup, keep
      wd_phase_epochs: []  # (N-1) the last phase goes to the end
      wd_phase_steps: null  # (N-1) the last phase goes to the end; if phase_steps is not null, override phase_epochs
      wd_phase_scales: [1.0, 1.0]  # (N+1) start scale of each phase and the final scale of the end
      wd_phase_types: [keep_start]  # (N)
    
  optimizer_2:
    identifier: fcs  # just the name for the module in the root_module
    optimizer_choice: adamw
    lr_default: 1.0e-4
    wd_default: 1.0e-4
    max_grad_norm_name: [all]  # names for grad norm groups in loggers
    max_grad_norm_value: [.inf]  # .inf means no gradient clipping (setting to 0.0 is the same, but grad_norm will not be logged)
    max_grad_norm_modules: [['']]  # nested_list: exact module name for this optimizer's root module, '' means the whole model, or a list of submodule names [submodule_name1, submodule_name2, ...]
    lr_scheduler:
      ### len(phase) == N, e.g. N=2: warmup, keep
      lr_phase_epochs: [1]  # (N-1) the last phase goes to the end
      lr_phase_steps: null  # (N-1) the last phase goes to the end; if phase_steps is not null, override phase_epochs
      lr_phase_scales: [0.0, 1.0, 1.0]  # (N+1) start scale of each phase and the final scale of the end
      lr_phase_types: [cosine, keep_start]  # (N)
    wd_scheduler:
      ### len(phase) == N, e.g. N=2: warmup, keep
      wd_phase_epochs: []  # (N-1) the last phase goes to the end
      wd_phase_steps: null  # (N-1) the last phase goes to the end; if phase_steps is not null, override phase_epochs
      wd_phase_scales: [1.0, 1.0]  # (N+1) start scale of each phase and the final scale of the end
      wd_phase_types: [keep_start]  # (N)

sweep:
  sweep_enabled: False
  sweep_params:  # use '//' as the connector for sub-params
    trainer//optimizer_1//lr_default: [2.0e-4, 4.0e-4]
