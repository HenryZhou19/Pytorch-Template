env:
  seed_with_rank: True
  cuda_deterministic: False
  find_unused_parameters: True
  amp:
    amp_enabled: True
    amp_mode: fp16  # fp16, bf16

info: # info & wandb
  start_time : TBD
  work_dir: TBD
  task_type: Train  # Pretrain, Train, Pretrain_Re, Train_Re, ...
  batch_info: TBD
  name_tags: [special.extra_name, model.model_choice, data.dataset, info.batch_info]
  wandb:
    wandb_enabled: True
    wandb_tags: [info.start_time]
    wandb_resume_enabled: False  # if True, wandb run will have the same ID when resuming an existing training work
    wandb_watch_model: False  # do not use
    wandb_watch_freq: 100
    wandb_buffer_time: 300  # seconds
  tensorboard:
    tensorboard_enabled: True
    tensorboard_graph: True
  iter_log_freq: 10  # <= 0 means only log when an epoch ends
  cli_log_freq: 1
  global_tqdm: True
  torchinfo: True  # print model info to log.txt
  print_param_names: True

model:
  ema:
    ema_enabled: False
    ema_type: EMA
    ema_beta: 0.9999
    ema_update_after_step: 100
    ema_update_every: 10
    ema_power: 0.75
    ema_primary_criterion: True

trainer:
  trainer_breath_time: 0.0
  trainer_choice: default
  resume: null
  pretrained_models: null  # None or a dict of pretrained models {name1: path1, name2: path2, ...}
  load_from_ema: True
  freeze_modules: []  # [submodule_name1, submodule_name2, ...]
  dist_eval: True
  eval_freq: 1  # <= 0 means only evaluate when all epochs end
  max_grad_norm: 0.0  # <= 0 means no gradient clipping
  grad_accumulation: 1  # positive integer (keep it '1' in most cases)
  grad_checkpoint: False
  checkpoint_save_interval: 1  # n, save the epoch n, 2n, ... (but the later one will overwrite the previous one)
  checkpoint_reserve_factor: 0  # m, 0 means no reserve (only the last k*n epoch will be kept on disk), otherwise the m*n, 2m*n, ... will be kept on disk
  scheduler:
    lr_min_factor: 0.0  # [0., 1.] start warmup from: lr_min_factor * lr, and anneal to lr_min_factor * lr. will be changed in cycle scheduler
    warmup_type: linear  # no_warmup, constant, linear, exponential, cosine